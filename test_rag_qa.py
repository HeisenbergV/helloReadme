#!/usr/bin/env python3
"""
æµ‹è¯•RAGé›†æˆçš„AIé—®ç­”åŠŸèƒ½
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

async def test_rag_qa():
    """æµ‹è¯•RAGé›†æˆçš„AIé—®ç­”åŠŸèƒ½"""
    try:
        print("ğŸ§ª æµ‹è¯•RAGé›†æˆçš„AIé—®ç­”åŠŸèƒ½...")
        
        # æµ‹è¯•é…ç½®
        from src.services.llm.config import LLMConfig
        print("âœ… é…ç½®å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥å¯ç”¨çš„æœåŠ¡
        enabled_services = LLMConfig.get_enabled_services()
        print(f"âœ… å¯ç”¨çš„æœåŠ¡: {enabled_services}")
        
        if not enabled_services:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return
        
        # æµ‹è¯•AIæœåŠ¡ç®¡ç†å™¨
        from src.services.llm import AIServiceManager
        from src.services.ai.vectorizer import Vectorizer
        
        ai_manager = AIServiceManager()
        vectorizer = Vectorizer()
        
        print("âœ… æœåŠ¡å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # åˆå§‹åŒ–
        success = await ai_manager.initialize()
        if success:
            print("âœ… AIæœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•å‘é‡åŒ–æœåŠ¡
            print("ğŸ”„ åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡...")
            vectorizer_success = await vectorizer.initialize()
            if vectorizer_success:
                print("âœ… å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                
                # æµ‹è¯•RAGæ£€ç´¢
                print("ğŸ§ª æµ‹è¯•RAGæ£€ç´¢...")
                question = "æˆ‘æƒ³åšä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°"
                similar_projects = await vectorizer.search_similar_projects(question, top_k=3)
                
                if similar_projects:
                    print(f"âœ… RAGæ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(similar_projects)} ä¸ªç›¸å…³é¡¹ç›®:")
                    for i, project in enumerate(similar_projects, 1):
                        print(f"   {i}. {project.get('name', 'Unknown')}")
                        print(f"      æè¿°: {project.get('description', 'No description')[:100]}...")
                        print(f"      ç›¸ä¼¼åº¦: {project.get('similarity', 'Unknown')}")
                else:
                    print("âš ï¸  RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³é¡¹ç›®")
                
                # æµ‹è¯•é—®ç­”åŠŸèƒ½
                print("\nğŸ§ª æµ‹è¯•é—®ç­”åŠŸèƒ½...")
                
                # æ„å»ºRAGä¸Šä¸‹æ–‡
                rag_context = ""
                if similar_projects:
                    rag_context = "åŸºäºæ‚¨çš„å‘é‡åº“ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³é¡¹ç›®ä¿¡æ¯ï¼š\n\n"
                    for i, project in enumerate(similar_projects, 1):
                        rag_context += f"{i}. **{project.get('name', 'Unknown')}**\n"
                        rag_context += f"   - æè¿°: {project.get('description', 'No description')}\n"
                        rag_context += f"   - è¯­è¨€: {project.get('language', 'Unknown')}\n"
                        rag_context += f"   - æ˜Ÿæ•°: {project.get('stars', 'Unknown')}\n"
                        rag_context += f"   - ç›¸ä¼¼åº¦: {project.get('similarity', 'Unknown')}\n\n"
                
                # æ„å»ºç³»ç»Ÿæç¤ºè¯
                system_message = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¼€æºé¡¹ç›®æ¨èä¸“å®¶ã€‚åŸºäºç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„é¡¹ç›®ä¿¡æ¯ï¼Œä½ éœ€è¦ï¼š

1. ç†è§£ç”¨æˆ·æƒ³è¦æ„å»ºçš„ç³»ç»Ÿç±»å‹
2. åˆ†ææä¾›çš„ç›¸å…³é¡¹ç›®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
3. æ¨èæœ€åˆé€‚çš„å¼€æºé¡¹ç›®
4. è§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™äº›é¡¹ç›®
5. æä¾›é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚æŠ€æœ¯æ ˆã€æ´»è·ƒåº¦ã€è®¸å¯è¯ç­‰ï¼‰
6. ç»™å‡ºå…·ä½“çš„ä½¿ç”¨å»ºè®®å’Œå®æ–½æ­¥éª¤
7. å¦‚æœæä¾›äº†å‘é‡åº“ä¸­çš„é¡¹ç›®ä¿¡æ¯ï¼Œä¼˜å…ˆæ¨èè¿™äº›é¡¹ç›®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå›ç­”è¦è¯¦ç»†ã€ä¸“ä¸šã€å®ç”¨ã€‚å¦‚æœæä¾›äº†ç›¸å…³é¡¹ç›®ä¿¡æ¯ï¼Œè¯·é‡ç‚¹åˆ†æè¿™äº›é¡¹ç›®ã€‚"""
                
                if rag_context:
                    system_message += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{rag_context}"
                
                # è°ƒç”¨AIæœåŠ¡
                response = await ai_manager.chat(
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": f"æˆ‘æƒ³åš{question}ï¼Œæœ‰ä»€ä¹ˆå¼€æºé¡¹ç›®æ¨èå—ï¼Ÿè¯·è¯¦ç»†åˆ†æå¹¶æ¨èåˆé€‚çš„é¡¹ç›®ã€‚"}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )
                
                if response.error:
                    print(f"âŒ é—®ç­”å¤±è´¥: {response.error}")
                else:
                    print(f"âœ… é—®ç­”æˆåŠŸ:")
                    print(f"   æ¨¡å‹: {response.model}")
                    print(f"   å›ç­”: {response.content[:300]}...")
                    if response.usage:
                        print(f"   ä½¿ç”¨é‡: {response.usage}")
                
                # å…³é—­å‘é‡åŒ–æœåŠ¡
                await vectorizer.close()
                print("âœ… å‘é‡åŒ–æœåŠ¡å·²å…³é—­")
                
            else:
                print("âŒ å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–å¤±è´¥")
            
            # å…³é—­AIæœåŠ¡
            await ai_manager.close()
            print("âœ… AIæœåŠ¡å·²å…³é—­")
            
        else:
            print("âŒ AIæœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
        
        print("\nğŸ¯ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•RAGé›†æˆçš„AIé—®ç­”åŠŸèƒ½...")
    print("=" * 60)
    
    asyncio.run(test_rag_qa())
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼")
